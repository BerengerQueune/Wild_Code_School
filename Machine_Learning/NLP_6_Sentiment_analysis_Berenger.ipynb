{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP 6: Sentiment analysis - Berenger.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nNLeCn7R0b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ffb3b3e-c8a5-4061-c7fe-f4bb0c09f0f5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from nltk.stem import SnowballStemmer\n",
        "import spacy\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('popular')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH2nz3uOSLqr",
        "outputId": "e7f15be0-e1c0-47bf-b041-f10386814a50"
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/BerengerQueune/wild_notebooks/main/Dataset/train.csv\")\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27481, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc3epbzTSV25"
      },
      "source": [
        "Keep only positive and negative tweets (so you exclude neutral). What is the percentage of positive/negative tweets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1Q6rm1bSXqL",
        "outputId": "c9b41a4c-1701-4841-c58a-4d8c55a4923d"
      },
      "source": [
        "df_positive_negative = df.loc[df[\"sentiment\"].isin([\"negative\", \"positive\"])]\n",
        "\n",
        "df_positive_negative[\"sentiment\"].value_counts(normalize=True)*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    52.447595\n",
              "negative    47.552405\n",
              "Name: sentiment, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVTHVQ9kTbeX"
      },
      "source": [
        "<b><font color='orange'>The percentage of positive tweets is 52,44% and negative tweets is 47,55%.</font></b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RkPjaBRcXgn"
      },
      "source": [
        "# df_positive_negative.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFhz5oMvyQHj"
      },
      "source": [
        "Create a function you call clean that takes a sentence as parameter (so a str text) and returns a text (str) of tokens after applying a stemmer or a lemmatizer, separated by spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DeYbB1uyQdp",
        "outputId": "79e0353b-c2c9-4192-ea92-f1ad744eb937"
      },
      "source": [
        "def clean(sentence):\n",
        "  newsentence = \"\"\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  sent_tokens = nlp(sentence)\n",
        "  for token in sent_tokens:   \n",
        "    newsentence = newsentence + \" \" + (token.lemma_)\n",
        "  print(newsentence)\n",
        "\n",
        "clean(\"You are better when I am well.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -PRON- be well when -PRON- be well .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53nboqSoyUYb"
      },
      "source": [
        "Get the list of English stopwords from NLTK, and copy it into a stopwordsenglish list. Complete your clean function so that it removes punctuation and stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hOxfbxHf6-4O",
        "outputId": "3276a748-b2d3-4d0c-b966-f4b46631254e"
      },
      "source": [
        "stopwordsenglish = nltk.corpus.stopwords.words(\"english\")\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def clean(sentence):\n",
        "  sentence = re.sub(r'[^\\w\\s]','',str(sentence))\n",
        "  sentence = nltk.word_tokenize(sentence.lower())\n",
        "  newsentence = \"\"\n",
        "  tokens_clean = []\n",
        "  for words in sentence:\n",
        "    if words not in stopwordsenglish:\n",
        "      tokens_clean.append(words)\n",
        "  tokens_clean2 = ' '.join([str(item) for item in tokens_clean])\n",
        "  sent_tokens = nlp(tokens_clean2)\n",
        "  for token in sent_tokens:\n",
        "    newsentence = newsentence + \" \" + (token.lemma_)\n",
        "  return newsentence\n",
        "\n",
        "clean(\"You are better when I am well.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' better well'"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6TviI75yU2A"
      },
      "source": [
        "Apply this clean function to the text column of your DataFrame. Store the result in a new clean column in the DataFrame. (The processing may take 2 or 3 minutes)\n",
        "You should now have a DataFrame that looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9D7w4N29WKt",
        "outputId": "e82addc9-8455-4341-e13c-fb124e7e1413"
      },
      "source": [
        "df_positive_negative[\"clean\"] = df_positive_negative[\"text\"].apply(clean)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Fv2uwAvtFzFO",
        "outputId": "bf29b875-0bb5-4d34-ae8b-6f82fa754fd1"
      },
      "source": [
        "df_positive_negative"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "      <td>sooo sad miss san diego</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "      <td>boss bullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "      <td>interview leave alone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "      <td>son could not put release already buy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6e0c6d75b1</td>\n",
              "      <td>2am feedings for the baby are fun when he is a...</td>\n",
              "      <td>fun</td>\n",
              "      <td>positive</td>\n",
              "      <td>2 a.m. feeding baby fun smile coo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27475</th>\n",
              "      <td>b78ec00df5</td>\n",
              "      <td>enjoy ur night</td>\n",
              "      <td>enjoy</td>\n",
              "      <td>positive</td>\n",
              "      <td>enjoy ur night</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27476</th>\n",
              "      <td>4eac33d1c0</td>\n",
              "      <td>wish we could come see u on Denver  husband l...</td>\n",
              "      <td>d lost</td>\n",
              "      <td>negative</td>\n",
              "      <td>wish could come see u denver husband lose job...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27477</th>\n",
              "      <td>4f4c4fc327</td>\n",
              "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
              "      <td>, don`t force</td>\n",
              "      <td>negative</td>\n",
              "      <td>-PRON- have wonder rake client make clear net...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27478</th>\n",
              "      <td>f67aae2310</td>\n",
              "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
              "      <td>Yay good for both of you.</td>\n",
              "      <td>positive</td>\n",
              "      <td>yay good enjoy break probably need hectic wee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27479</th>\n",
              "      <td>ed167662a5</td>\n",
              "      <td>But it was worth it  ****.</td>\n",
              "      <td>But it was worth it  ****.</td>\n",
              "      <td>positive</td>\n",
              "      <td>worth</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16363 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           textID  ...                                              clean\n",
              "1      549e992a42  ...                            sooo sad miss san diego\n",
              "2      088c60f138  ...                                      boss bullying\n",
              "3      9642c003ef  ...                              interview leave alone\n",
              "4      358bd9e861  ...              son could not put release already buy\n",
              "6      6e0c6d75b1  ...                  2 a.m. feeding baby fun smile coo\n",
              "...           ...  ...                                                ...\n",
              "27475  b78ec00df5  ...                                     enjoy ur night\n",
              "27476  4eac33d1c0  ...   wish could come see u denver husband lose job...\n",
              "27477  4f4c4fc327  ...   -PRON- have wonder rake client make clear net...\n",
              "27478  f67aae2310  ...   yay good enjoy break probably need hectic wee...\n",
              "27479  ed167662a5  ...                                              worth\n",
              "\n",
              "[16363 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIb2FxHnyYvk"
      },
      "source": [
        "Copy the clean column into a Series X, and the sentiment column into a Series y. Apply a split test-train with the training set size at 0.75 with random_state = 32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JSHON-VI6oo"
      },
      "source": [
        "X = df_positive_negative[\"clean\"]\n",
        "y = df_positive_negative['sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 32, train_size = 0.75)\n",
        "\n",
        "def classification_models(train, test):\n",
        "  \n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    model = KNeighborsClassifier(n_neighbors=2)\n",
        "    model.fit(X_train_CV, y_train)\n",
        "    print(\"Score for the KNN Train dataset :\", model.score(train, y_train))\n",
        "    print(\"Score for the KNN Test dataset :\", model.score(test, y_test))\n",
        "\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    modelLR = LogisticRegression()\n",
        "    modelLR.fit(X_train_CV, y_train)\n",
        "    print(\"Score for the Logistic Regression Train dataset :\", modelLR.score(train, y_train))\n",
        "    print(\"Score for the Logistic Regression Test dataset :\", modelLR.score(test, y_test))\n",
        "\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    modelDTC = DecisionTreeClassifier()\n",
        "    modelDTC.fit(X_train_CV, y_train)\n",
        "    print(\"Score for the Decision Tree Classifier Train dataset :\", modelDTC.score(train, y_train))\n",
        "    print(\"Score for the Decision Tree Classifier Test dataset :\", modelDTC.score(test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7tQAKMuyYog"
      },
      "source": [
        "Apply a CountVectorizer and train classification models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qh6aPRXKJxJ",
        "outputId": "571b6c97-75bb-4668-ecd9-0739d5c92e1e"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(X_train)\n",
        "\n",
        "X_train_CV = vectorizer.transform(X_train)\n",
        "\n",
        "X_test_CV = vectorizer.transform(X_test)\n",
        "\n",
        "classification_models(X_train_CV, X_test_CV)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for the KNN Train dataset : 0.8334419817470665\n",
            "Score for the KNN Test dataset : 0.7191395746761183\n",
            "Score for the Logistic Regression Train dataset : 0.9565677966101694\n",
            "Score for the Logistic Regression Test dataset : 0.8618919579564899\n",
            "Score for the Train dataset : 0.9995110821382008\n",
            "Score for the Test dataset : 0.8203373258372036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrA6YY4ByYU3"
      },
      "source": [
        "Apply a TfidfVectorizer and train classification models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fsq_J0CKVIN",
        "outputId": "6e58a0ee-3ce3-4ecc-c54b-8f2e5c5ec5ee"
      },
      "source": [
        "tfidf = TfidfVectorizer()\n",
        "tfidf.fit(X_train)\n",
        "\n",
        "X_train_CV = tfidf.transform(X_train)\n",
        "\n",
        "X_test_CV = tfidf.transform(X_test)\n",
        "\n",
        "classification_models(X_train_CV, X_test_CV)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for the KNN Train dataset : 0.5945241199478487\n",
            "Score for the KNN Test dataset : 0.5678318259594232\n",
            "Score for the Logistic Regression Train dataset : 0.9295958279009127\n",
            "Score for the Logistic Regression Test dataset : 0.8631141530188218\n",
            "Score for the Train dataset : 0.9995110821382008\n",
            "Score for the Test dataset : 0.8188706917624052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4poRJqVfydZ4"
      },
      "source": [
        "Compare the scores, which parameters give the best scores?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PTZuHsYNcBQ"
      },
      "source": [
        "<b><font color='orange'>I think the Logistic Regression with a CountVectorizer is giving the best score.</font></b>"
      ]
    }
  ]
}